{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6155d1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [torch]m 9/10\u001b[0m [torch]]x]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 setuptools-80.9.0 sympy-1.14.0 torch-2.8.0 typing-extensions-4.15.0\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e8841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deadmarkov/Transformer-From-Scratch-1/.venv/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Defining Embedding class\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInputembedding\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# defining constructor\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,d_model:\u001b[38;5;28mint\u001b[39m,vocab_size:\u001b[38;5;28mint\u001b[39m):\n\u001b[32m     11\u001b[39m         \u001b[38;5;28msuper\u001b[39m()._init_()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch.nn' has no attribute 'module'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Defining Embedding class\n",
    "\n",
    "class Inputembedding(nn.Module):\n",
    "    # defining constructor\n",
    "\n",
    "    def __init__(self,d_model:int,vocab_size:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model= d_model # dimension of the word vector\n",
    "        self.vocab_size= vocab_size # total number of word in vocubalory\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) *  math.sqrt(self.d_model)\n",
    "\n",
    " # positional encoding   \n",
    "class positionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model:int,seq_len:int,dropout:float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model= d_model\n",
    "        self.seq_len= seq_len\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        pe= torch.zeros(seq_len,d_model) # create a matrix of shape. (seq_len,d_model)\n",
    "        position= torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1) # position shape will be (seq_len,1) it will show position of each word\n",
    "\n",
    "        div_term= torch.exp(torch.arange(0,d_model,2).float()* (-math.log(10000)/d_model))\n",
    "        # apply the sin to the even position word\n",
    "        pe[:,0::2]= torch.sin(position*div_term)\n",
    "        pe[:,1::2]= torch.cos(position*div_term)\n",
    "        # Batch dimension = number of sentences (or sequences) processed in parallel.\n",
    "        # Positional encoding is shared across all sentences in the batch.\n",
    "        pe= pe.unsqueeze(0) # so it will be dimensioin (1,seq_len,d_model)\n",
    "        self.register_buffer('pe',pe) # add into register buffer \n",
    "\n",
    "    def forward(self,x):\n",
    "        x= x + (self.pe[:,:x.shape[1],:]).requieres_grad(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d458a15",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.7' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ad5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
